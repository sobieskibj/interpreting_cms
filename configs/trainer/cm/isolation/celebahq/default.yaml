_partial_: true
_target_: experiment.cm.training.Trainer

diffusion: ${diffusion}

ema_scale_fn: 
  _target_: model.cm.src.script_util.create_ema_and_scales_fn
  target_ema_mode: adaptive
  start_ema: 0.95
  scale_mode: progressive
  start_scales: 2
  end_scales: 150
  total_steps: ${exp.total_training_steps}
  distill_steps_per_iter: 50,000

schedule_sampler:
  _target_: model.cm.src.resample.create_named_schedule_sampler
  name: uniform
  diffusion: ${diffusion}

ema_rate: [0.9999, 0.99994, 0.9999432189950708]

log_interval: 10
save_interval: 2500

resume_checkpoint:

use_fp16: true
fp16_scale_growth: 0.001
lg_loss_scale: 20.0

lr_anneal_steps: 0
weight_decay: 0.0
lr: 0.00005

microbatch: 2

step: 0
resume_step: 0
global_step: 0
training_mode: consistency_training